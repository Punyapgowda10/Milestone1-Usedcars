{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone :1 Predicting the Demand for Used Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an automobile company XYZ from USA which aspires to enter the US used car market by setting up their company locally to give competition to their counterparts. They want to understand the factors affecting the pricing of cars in the market, since those may be vastly different from the new car market.\n",
    "       \n",
    "The market for used cars has witnessed a significant surge in demand in recent times, surpassing that of new cars. Consequently, pricing strategies for used vehicles have become crucial for businesses to remain competitive in the market. The value of a used car is influenced by a multitude of factors, such as mileage, model, and year of production. It is imperative for industry players to consider these variables while determining the actual worth of a used car.\n",
    "      \n",
    "First step document that lists the output of your exploratory analysis, any issues, or problems you may see with data that need follow-up, and some basic descriptive analysis that you think highlights important outcomes/findings from the data. Based on your findings, the next level of analysis will be charted out. Build a multiple linear regression model for predicting the price of a used car and provide meaningful inferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset: Used_cars_sales.xlsx**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Dictionary**\n",
    "\n",
    "    Name \t                 Name of the Car on Sell \n",
    "    City \t                 City of the Car on Sell \n",
    "    Years             \t     Year of manufacture \n",
    "    Km_driven  \t             Kms Car travelled \n",
    "    Fuel_Type  \t             Petrol, Diesel, CNG \n",
    "    Transmission  \t         Manual, Automatic \n",
    "    Owner_Type     \t         First, Second, Third \n",
    "    Mileage  \t             Mileage of the Car \n",
    "    Engine \t                 Engine Displacement \n",
    "    Power             \t     Power of Engine \n",
    "    Seats              \t     No. of Seats in Car \n",
    "    Selling_price   \t     Selling price of a Car (Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Guidelines:**\n",
    "\n",
    "Ensure to follow to Use Idâ€™s provided by UNext for naming file as conventions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Instructions \n",
    "\n",
    "- The assessment has 16 questions, each question is a separate function\n",
    "- The cells in the Jupyter notebook can be executed any number of times for testing the solution\n",
    "- Refrain from modifying the boilerplate code as it may lead to unexpected behavior \n",
    "- The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
    "- On completing all the questions, the assessment is to be submitted on moodle for evaluation\n",
    "- The kernel of the Jupyter notebook is to be set as `Python 3 (ipykernel)` if not set already\n",
    "- Include imports as necessary\n",
    "- For each of the task, `Note` section will provide you hints to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Utilize software engineering aspects while building Machine learning model using modular programming principles to organize your code into reusable functions or classes to enhance readability, maintainability, and collaboration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Import various libraries and modules used in data analysis, machine learning, and visualization tasks in Python such as `pandas`, `numpy`, `sklearn`, `matplotlib`, `seaborn`, `statsmodels`, `statsmodels.api`, `sklearn.model_selection`, `sklearn.linear_model`, `sklearn.metrics`, `sklearn.preprocessing`. There are 2 ways to import the libraries and modules:\n",
    "* import numpy as np\n",
    "* from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load the 'car_sales-dataset' available in the Dataset folder within the Project folder on the desktop. Perform preliminary EDA with key observations and insights- (weightage - 40 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.1: Load the Used_cars_sales dataset using try and except blocks.          (weightage - 2 marks) (AE)        \n",
    "\n",
    "#### NOTE:\n",
    "The `read_excel` method in Pandas allows you to read Excel files and convert them into a DataFrame, which is a two-dimensional tabular data structure in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_the_dataset(dataset_location : str)->pd.DataFrame:\n",
    "    car_sales = None\n",
    "    # code starts here\n",
    "    \n",
    "    # code ends here\n",
    "    return car_sales    \n",
    "# store the result of the dataset\n",
    "dataset_location = 'Used_car_sales.xlsx'\n",
    "car_sales=load_the_dataset(dataset_location)\n",
    "print(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.2: Which brand is selling the most in terms of the number of units sold    (weightage - 2 marks)  (AE)  \n",
    "\n",
    "**Hint: output should be the name of the brand only.**\n",
    "\n",
    "#### NOTE:\n",
    "To count each unique value in the 'name' column of the DataFrame car_sales we can use the value_counts() method. It returns a Series where the index contains unique car names, and the values represent the frequency of each name in the DataFrame.\n",
    "To find the name of the car brand with the maximum count we can use the idxmax() method on the series. It returns the name of the most selling brand in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_value_counts(car_sales):\n",
    "    most_selling_brand = None\n",
    "    #code starts here\n",
    "    \n",
    "\n",
    "    #code ends here\n",
    "    return most_selling_brand\n",
    "get_name_value_counts(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.3: Name the top 5 cities with the highest number of cars sold.. (weightage - 2 marks)        (AE)\n",
    "\n",
    "**Hint: Output should be city name only in the format of list**\n",
    "\n",
    "#### NOTE:\n",
    "To calculate the count of each unique value in the 'City' column of the DataFrame data use the value_counts() method. It will returns a Series where the index contains unique city names, and the values represent the frequency of each city in the DataFrame.\n",
    "Now to select the top 5 cities with the highest sales count use the head() method on the Series. By default, head() returns the first 5 rows of the Series, which in this case, correspond to the cities with the highest sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_value_counts(data):\n",
    "    city_with_max_sales = None\n",
    "    #code starts here\n",
    "    \n",
    "    #code ends here\n",
    "    return city_with_max_sales.head()\n",
    "# Remove 'name', 'count', and 'dtype' information   \n",
    "city_name=get_city_value_counts(car_sales)\n",
    "list(city_name.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.4: Check missing values in the data in terms of percentage using error handling technique and do missing value treatment.  (weightage - 4 marks)       (AE) \n",
    "\n",
    "#### NOTE:\n",
    "Find the percentage of missing values in the data by dividing the total number of missing values by the total number of rows and multiplying by 100, you will get the percentage of missing values for each column. \n",
    "\n",
    "Use `isnull().sum()` to calculate the total number of missing values in each column and `shape[0]` to get the total number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_check(data):\n",
    "    missing_percentage = None\n",
    "    #code starts here\n",
    "    try:\n",
    "        # Check for missing values\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        return 'An error occurred'\n",
    "\n",
    "    #code ends here\n",
    "    return missing_percentage\n",
    "missing_value_check(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "For treating the missing value first define the list of columns containing the names of the columns with missing values which will be treated. Iterate over each column in the list:\n",
    "\n",
    "If the column is numerical, fill the missing values using the mean or median of that column (based on suitability).\n",
    "\n",
    "If the column is categorical, fill the missing values using the mode (most frequent value) of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value treatment\n",
    "def missing_value_treatment(car_sales):\n",
    "    # Example: Replace missing values with mode value\n",
    "    #code starts here\n",
    "\n",
    "\n",
    "\n",
    "    # code ends here\n",
    "        \n",
    "    return car_sales\n",
    "car_sales = missing_value_treatment(car_sales)\n",
    "print(car_sales.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.5: Detect Outliers in the data and do outlier treatment. Plot a boxplot to visualize outliers in the data. (weightage â€“ 6 marks)        (AE)     \n",
    "* The extreme values in selling_price due to genuine market conditions, such as luxury properties.Keep the values as it is.\n",
    "\n",
    "#### NOTE:\n",
    "The `sns.boxplot()` function in Seaborn is used to create a box plot visualization, which is a convenient way to visually summarize the distribution of numerical data and identify outliers. Using this detect the outliers in the `car_sales` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a boxplot to visualize outliers in the data\n",
    "#car_sales = car_sales.drop('Sales_ID', axis=1)\n",
    "def box_plot1(data):\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return dta\n",
    "box_plot1(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function `treat_outliers_iqr` to treat outliers using IQR method. Use `median` as treatment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "The outliers in the dataset can be treated using the Interquartile Range (IQR) method. It replaces the outliers with either the median or the mean, based on the specified treatment method. Calculate the first quartile (Q1), third quartile (Q3), and Interquartile Range (IQR) of the data along with the lower and upper bounds for outliers. Then replace the outliers with the median of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect and treat outliers using IQR method\n",
    "def treat_outliers_iqr(data,treatment='median'):\n",
    "    #code starts here\n",
    "    \n",
    "    \n",
    "    data = data.copy()  # Avoid modifying the original DataFrame slice\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "    return data\n",
    "# Detect outliers and treat them with median value\n",
    "numeric_columns=['km_driven']\n",
    "# Display the treated DataFrame\n",
    "\n",
    "#code ends here\n",
    "# Display the treated DataFrame\n",
    "print(car_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a boxplot to visualize outliers in the data\n",
    "#car_sales = car_sales.drop('Sales_ID', axis=1)\n",
    "def box_plot2(data):\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return \n",
    "box_plot2(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.6: Which variables are significant in predicting the price of a used car? To answer this question, it is important to understand the correlation between the different variables. i.e how much the other features affect the selling price of a used car. (Bivariate analysis) (weightage - 3 marks)               (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "To understand the correlation between different variables , we have to convert the non numeric columns into numeric columns. Clean the data type of the 'max_power', 'engine', and 'mileage' columns in a DataFrame by converting them into numeric types by removing the units in order to perform correlation on them.\n",
    "\n",
    "Removes the following non-numeric suffix using string manipulation functions:\n",
    "* 'bhp' for max_power, \n",
    "* 'CC' for engine, \n",
    "* 'kmpl' or 'km/kg' for mileage\n",
    "\n",
    "Converts the data type of each of the above column to float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert data type of Max power, Engine and Mileage into numeric\n",
    "def clean_car_sales_data(df):\n",
    "    #code starts here\n",
    "\n",
    "    \n",
    "    #code ends here\n",
    "    return df\n",
    "car_sales = clean_car_sales_data(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical variables and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Using Python function `corr` compute the correlation matrix of a DataFrame df containing numeric columns.\n",
    "Ensure that only numeric columns are included in the correlation computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(df):\n",
    "    correlation = None\n",
    "    #code starts here\n",
    "    \n",
    "    #code ends here\n",
    "    return correlation\n",
    "corr_data = corr(car_sales)\n",
    "corr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.7: Come up with Insights and validate the hypothesis:                         (weightage: 15 marks)  (AE)\n",
    "\n",
    "(i) Is there a statistically significant difference in the average mileage of cars across different fuel types and transmission types?\n",
    "\n",
    "(ii) Is there a statistically significant difference in the selling prices of cars between individual and dealer sellers?\n",
    "\n",
    "(iii)Is there a relationship between the type of fuel and transmission in cars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Is there a statistically significant difference in the average mileage of cars across different fuel types and transmission types?\n",
    "\n",
    "- If yes,print :\"There is a significant difference in the average mileage of cars across different fuel types and transmission types\"\n",
    "- If no, print: There is no significant difference in the average mileage of cars across different fuel types and transmission types\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Conduct an Analysis of Variance (ANOVA) test to determine if there is a significant difference in the average mileage of cars across different fuel types and transmission types.\n",
    "* Groups the data to create separate groups for each category.\n",
    "* Then perform the ANOVA test using the stats.f_oneway() function from the scipy.stats module.\n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison, determine whether there is a significant difference in the average mileage of cars across different fuel types and transmission types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def perform_anova(car_data):\n",
    "    result1 = None\n",
    "    # Code starts here\n",
    "\n",
    "    # Perform ANOVA\n",
    "    \n",
    "\n",
    "    # Compare p-value with significance level (e.g., 0.05)\n",
    "\n",
    "    \n",
    "    # Code ends here\n",
    "    \n",
    "    # Return ANOVA results\n",
    "    return result1\n",
    "# Call the function\n",
    "perform_anova(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Is there a statistically significant difference in the selling prices of cars between individual and dealer sellers?\n",
    "\n",
    "- If yes,print : \"There is a statistically significant difference in the selling prices of cars between individual and dealer sellers\"\n",
    "- If no, print: \"There is no statistically significant difference in the selling prices of cars between individual and dealer sellers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Conducts an independent two-sample t-test to determine if there is a statistically significant difference in the selling prices of cars between individual and dealer sellers.\n",
    "* Filter the data based on the 'seller_type' column to separate the selling prices of cars sold by individual sellers and dealer sellers.\n",
    "* Perform the independent two-sample t-test using the ttest_ind() function from the scipy.stats module. \n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison,determines whether there is a statistically significant difference in the selling prices of cars between individual and dealer sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "def seller_type_influence_test(car_sales):\n",
    "    result2 = None\n",
    "    # Code starts here\n",
    "    # Filter data based on seller type\n",
    "    \n",
    "\n",
    "    # Perform independent t-test\n",
    "   \n",
    "\n",
    "    # Set significance level\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Interpret the results\n",
    "    \n",
    "    \n",
    "    # Code ends here    \n",
    "    return result2\n",
    "seller_type_influence_test(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Is there a relationship between the type of fuel and transmission in cars?\n",
    "\n",
    "- If yes,print : \"There is a statistically significant relationship between the type of fuel and transmission in cars\"\n",
    "- If no, print: \"There is no statistically significant relationship between the type of fuel and transmission in cars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Conduct a chi-square test of independence to determine if there is a statistically significant relationship between the type of fuel and transmission in cars.\n",
    "* Creates a contingency table of observed frequencies using the pd.crosstab() function. \n",
    "* Perform the chi-square test of independence using the stats.chi2_contingency() function from the scipy.stats module. \n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison, determine whether there is a statistically significant relationship between the type of fuel and transmission in cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(car_data):\n",
    "    result3 = None\n",
    "    #code starts here\n",
    "    # Create a contingency table of observed frequencies\n",
    "    \n",
    "\n",
    "    # Perform Chi-square test\n",
    "   \n",
    "\n",
    "    # Compare p-value with significance level (e.g., 0.05)\n",
    "    \n",
    "    # code ends here\n",
    "    # Return Chi-square statistic, p-value, and result\n",
    "    return result3\n",
    "\n",
    "# Call the function\n",
    "chi_square_test(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.8: Data Transformation:  remove the given variables for model building process (\"name\",\"Postal_code\",\"Sales_ID\"). (weightage - 2 marks)        (AE)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Drop the varaiables \"name\",\"Postal_code\",\"Sales_ID\" from the dataframe using the drop() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the given variables\n",
    "def drop_var1(data):\n",
    "    # Code starts here\n",
    "    \n",
    "    # Code ends here\n",
    "    return data\n",
    "car_sales = drop_var1(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.9: Handling categorical features: Apply encoding technique to convert categorical variable into numerical. Use try and except blocks.  (weightage - 4 marks)    (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "Transform city variable to 4 categories such as `North_city, South_city, East_city and West_city` according to the below given list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions\n",
    "def city_region(sales_data):\n",
    "    north_cities = ['Dehradun','Mumbai','Jaipur','Indore','Pune','Aurangabad', 'Delhi','Ludhiana', 'kanpur', 'Gangtok', 'Noida']\n",
    "    south_cities = ['Chennai','Mysore', 'Bangalore', 'Nellore', 'Coimbatore','Mangalore','Hyderabad','Vellore','Thrissur']\n",
    "    east_cities = ['Ranchi', 'Kolkata', 'Jamshedpur', 'Patna', 'Bhubaneshwar']\n",
    "    west_cities = ['Ahmedabad','Kochi','Vadodara', 'Surat']\n",
    "\n",
    "    # Transform the City variable to 4 categories\n",
    "    #code starts here\n",
    "\n",
    "\n",
    "\n",
    "    # code ends here\n",
    "    \n",
    "    # Print the updated DataFrame\n",
    "    return sales_data\n",
    "#apply city_region() in the original data\n",
    "car_sales=city_region(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop the City variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_var2(data):\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return data\n",
    "car_sales = drop_var2(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding technique: convert other categorical variable into factor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_categorical_columns(df):    \n",
    "    try:\n",
    "        # Initialize LabelEncoder\n",
    "        # code starts here\n",
    "\n",
    "        # Label encode selected columns\n",
    "        label_encode_columns = ['fuel', 'seller_type', 'transmission', 'owner', 'Sales_status']\n",
    "        \n",
    "\n",
    "        # One-hot encode 'City_region'\n",
    "        \n",
    "        #code ends here\n",
    "    except Exception as e:\n",
    "        return \"An error occurred during encoding:\"\n",
    "\n",
    "    return df\n",
    "# apply it in the original data\n",
    "car_sales=encode_categorical_columns(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Save the DataFrame car_sales to an Excel file named __\"cleaned_car_sales.xlsx\"__ in the `Project1` folder without including the index column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data):\n",
    "    try:\n",
    "        # code starts here\n",
    "\n",
    "        # code ends here\n",
    "        print(\"Write operation successfully completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Write operation not completed.\")\n",
    "        print(f\"Error: {e}\")\n",
    "save_file(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Build Multiple Linear regression model for predicting the price of a used car. (weightage - 30 marks)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.1: Load the cleaned dataset and divide it into predictor and target values (X & y) (weightage â€“ 2 marks) (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned data\n",
    "def load_the_cleaned_dataset(dataset_location):\n",
    "        cleaned_car_sales = None\n",
    "        # Code starts here\n",
    "        \n",
    "        # Code ends here\n",
    "        return cleaned_car_sales\n",
    "dataset_location = 'cleaned_car_sales.xlsx'\n",
    "cleaned_car_sales=load_the_cleaned_dataset(dataset_location)    \n",
    "print(\"cleaned_car_sales.xlsx load complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate independent features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate independent features and target variable\n",
    "def separate_data_and_target(df):\n",
    "    X, y = np.ndarray([]),np.ndarray([])\n",
    "    #code starts here\n",
    "    \n",
    "\n",
    "   \n",
    "    #code ends here\n",
    "    return X,y\n",
    "# dependent : X, independent : y\n",
    "X, y = separate_data_and_target(cleaned_car_sales)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.2: Split the dataset into train and test in the ratio of 80:20 and apply the scaling technique -RobustScaler. (weightage â€“ 4 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "Split the dataset into training and testing. Separate the dependent (data) and independent (target) columns/fields of the dataset and apply the scaling technique -RobustScaler.The requirement is to build a model that performs regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def split_into_train_and_test_normalize_features(X,y):\n",
    "    X_train, X_test,y_train,y_test = np.ndarray([]),np.ndarray([]),np.ndarray([]),np.ndarray([])\n",
    "\n",
    "    #code starts here\n",
    "    # Splitting dataset to train and test sets 80% train and 20% test  (random_state=0)\n",
    "     \n",
    "\n",
    "        \n",
    "    #Normalizes selected features in a dataset \n",
    "    # Instantiate the RobustScaler\n",
    "    \n",
    "\n",
    "    # Fit the scaler to the training data and transform it\n",
    "       \n",
    "\n",
    "    # Transform the testing data using the same scaler\n",
    "    \n",
    "   # code ends here\n",
    "    return X_train, X_test,y_train,y_test\n",
    "X_train, X_test,y_train,y_test=split_into_train_and_test_normalize_features(X,y)\n",
    "print(\"--------X_train-----------\")\n",
    "print(X_train)\n",
    "print(\"--------X_test-----------\")\n",
    "print(X_test)\n",
    "print(\"--------y_train-----------\")\n",
    "print(y_train)\n",
    "print(\"--------y_test-----------\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### T2.3 Check the multicollinearity using VIF for all the variables in the model (weightage-4 marks)  (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Splits the data into training and testing sets using the train_test_split function from scikit-learn. \n",
    "* Define the dependent variable (y1) and independent variables (X1) using the `dmatrices` function from the `patsy` module. \n",
    "* For each independent variable in X1, it calculates the VIF using the variance_inflation_factor function from the statsmodels.stats.outliers_influence module. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated.\n",
    "* Store the calculated VIF values along with the corresponding feature names in a DataFrame `vif` to display the VIF values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "def multicollinearity(data): \n",
    "\n",
    "    # code starts here\n",
    "    # random_state=1234\n",
    "    \n",
    "    y1, X1 = dmatrices('selling_price~year+km_driven+max_power+engine+seats+fuel+seller_type+Sales_status+mileage+transmission+owner',train, return_type='dataframe')\n",
    "\n",
    "    # For each X, calculate VIF and save in dataframe\n",
    "\n",
    "    # code ends here\n",
    "    print(vif.round(1))\n",
    "    return\n",
    "multicollinearity(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.4: Build a multiple linear regression model on training data using OLS method and sklearn library . (weightage - 20 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Split the dataset into train and test in the ratio of 80:20.\n",
    "* Set the random_state to 1234\n",
    "* Build the OLS model using statsmodels formula API\n",
    "* The function `fit_the_model_ols` should return the OLS Model and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a model using OLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "def fit_the_model_ols(data):    \n",
    "    ols_model = None\n",
    "    train, test = np.ndarray([]),np.ndarray([])\n",
    "    # Code starts here\n",
    "    #random state=1234\n",
    "\n",
    "    \n",
    "    # Building the OLS model using statsmodels formula API\n",
    "\n",
    "    \n",
    "     # code ends here   \n",
    "    # Return the model and the test data    \n",
    "    return ols_model,test\n",
    "# Fit the model\n",
    "ols_model, test_data = fit_the_model_ols(cleaned_car_sales)\n",
    "print(\"-----------test_data---------\")\n",
    "print(test_data)\n",
    "# Get the summary of the fitted model\n",
    "summary = ols_model.summary()\n",
    "# Print the summary\n",
    "print(\"------Model Summary ---------\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear model using sklearn library** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Initializes a linear regression model object.\n",
    "* Fit the model to the training data (X_train and y_train) using the fit() method of the linear regression object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model_on_the_training_data(data,X_train:np.ndarray,y_train:np.ndarray):\n",
    "    regression = None\n",
    "    # Code starts here\n",
    "\n",
    "    \n",
    "    # Code ends here \n",
    "    return regression\n",
    "# train the training data \n",
    "sklearn_model=fit_the_model_on_the_training_data(cleaned_car_sales,X_train,y_train)\n",
    "sklearn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Evaluate the performance of the model using the right evaluation metrics. (weightage - 25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.1: Evaluate the linear regression model with evaluation metrics R2 and RMSE using OLS and sklearn library (weightage - 10 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the OLS model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "* Make the predictions on the test set using the provided OLS linear regression model \n",
    "* Calculate the RMSE by comparing the predicted selling prices with the actual selling prices in the test set, using the mean_squared_error function from scikit-learn.\n",
    "* Calculate the R-squared (RÂ²) value using the rsquared attribute of the linear regression model.\n",
    "* Return both `RMSE` and `RÂ²` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def ols_calculate_rmse_r2(model, test_data):\n",
    "    rmse,r_squared = 0.0,0.0\n",
    "\n",
    "    # code starts here\n",
    "    # Making predictions on the test set\n",
    "    \n",
    "\n",
    "    # Calculating RMSE\n",
    "   \n",
    "    # code ends here\n",
    "    return rmse,r_squared\n",
    "ols_rmse, ols_r_squared = ols_calculate_rmse_r2(ols_model, test_data)\n",
    "# OLS Model RMSE value\n",
    "print(\"-----------ols_rmse-----------\")\n",
    "print (ols_rmse)\n",
    "# OLS R2 value\n",
    "print(\"--------------ols_r_squared----------\")\n",
    "print (ols_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Sklearn model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Make the predictions using a trained scikit-learn model (model) on a test dataset (X_test)\n",
    "* Using the trained model predict the target variable for the provided test dataset (X_test) using the predict() method of the model.\n",
    "* Return the predicted target variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_the_sklearn_model(model, X_test: np.ndarray) -> np.ndarray:\n",
    "    y_prediction = np.ndarray([])\n",
    "    # code starts here\n",
    "   \n",
    "    # code ends here\n",
    "    return y_prediction\n",
    "y_pred=test_the_sklearn_model(sklearn_model,X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model with evaluation metric R2 and RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "def calculate_r_squared_sklearn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        R-squared value.\n",
    "    \"\"\"\n",
    "    r_squared = None\n",
    "    # Code starts here\n",
    "\n",
    "    \n",
    "    # Code ends here\n",
    "    return r_squared\n",
    "# Calculate R-squared   [ RMSE on training]\n",
    "r_squared = calculate_r_squared_sklearn(y_test, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_sklearn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        RMSE value.\n",
    "    \"\"\"\n",
    "    rmse = None\n",
    "    # Code starts here\n",
    "\n",
    "    \n",
    "    # Code ends here     \n",
    "    return rmse\n",
    "# Calculate RMSE\n",
    "rmse_test = calculate_rmse_sklearn(y_test, y_pred)\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.2:Remove the insignificant variables which identified by OLS and multicollinearity -VIF technique. Then apply log transformation on target variable and build the model using sklearn library and evaluate it with R2 and RMSE(weightage - 15 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove insignificant variable**\n",
    "#### NOTE:\n",
    "* Because of multicollinearity issue drop the variable engine (VIF > 5)\n",
    "* Because of insignificance drop the variables - City_region, owner, Sales_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "# Create the residuals vs. fitted values plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred, y=residuals, alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs. Fitted Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Perform Shapiro-Wilk test\n",
    "stat, p_value = shapiro(residuals)\n",
    "\n",
    "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value > 0.05:\n",
    "    print(\"Residuals appear to be normally distributed (fail to reject H0).\")\n",
    "else:\n",
    "    print(\"Residuals do not appear to be normally distributed (reject H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_variables(data):\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return data\n",
    "cleaned_car_sales=drop_variables(cleaned_car_sales)\n",
    "cleaned_car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Log transformation on target variable**\n",
    "\n",
    "#### NOTE:\n",
    "* Transform the target variable 'selling_price' into its natural logarithm ('ln_selling_price') using the numpy log function\n",
    "* Return the ln_selling_price' column, which contains the transformed target variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_transform(data):\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return data[\"ln_selling_price\"]\n",
    "target_transform(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate independent features and target variable**\n",
    "#### NOTE:\n",
    "* Extract the feature variables (X) by dropping the columns 'ln_selling_price' and 'selling_price' from the DataFrame df using the drop() method\n",
    "* Extract the target variable (y) by selecting only the 'ln_selling_price' column from the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data_and_target_new(df):\n",
    "    X,y = np.ndarray([]),np.ndarray([])\n",
    "    # Code starts here\n",
    "\n",
    "    \n",
    "    # Code ends here\n",
    "    return X,y\n",
    "X, y = separate_data_and_target_new(cleaned_car_sales)\n",
    "print(\"---------X----------\")\n",
    "print(X)\n",
    "print(\"----------y---------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_and_test_normalize_features_2(X,y):\n",
    "    X_train, X_test,y_train,y_test = np.ndarray([]),np.ndarray([]),np.ndarray([]),np.ndarray([])\n",
    "    # Splitting dataset to train and test sets 80% train and 20% test (random _state=0)\n",
    "    # code starts here\n",
    "\n",
    "        \n",
    "    #Normalizes selected features in a dataset \n",
    "    # Instantiate the RobustScaler\n",
    "    \n",
    "\n",
    "    # Fit the scaler to the training data and transform it\n",
    "    \n",
    "\n",
    "    # Transform the testing data using the same scaler\n",
    "    \n",
    "    # code ends here\n",
    "    return X_train, X_test,y_train,y_test\n",
    "# split into training and testing \n",
    "X_train, X_test,y_train,y_test=split_into_train_and_test_normalize_features_2(X,y)\n",
    "print(\"--------X_train-----------\")\n",
    "print(X_train)\n",
    "print(\"--------X_test-----------\")\n",
    "print(X_test)\n",
    "print(\"--------y_train-----------\")\n",
    "print(y_train)\n",
    "print(\"--------y_test-----------\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model_on_the_training_data_2(data,X_train:np.ndarray,y_train:np.ndarray):\n",
    "    regression = None\n",
    "    # Code starts here\n",
    "\n",
    "    \n",
    "    # Code ends here \n",
    "    return regression\n",
    "final_model=fit_the_model_on_the_training_data_2(cleaned_car_sales,X_train,y_train)\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model on the testing dataset and return the predicted values for the test dataset\n",
    "def test_the_finalmodel(model1, X_test): \n",
    "    y_prediction = np.ndarray([])\n",
    "    # code starts here\n",
    "\n",
    "    \n",
    "    # code ends here\n",
    "    return y_prediction\n",
    "#prediction\n",
    "y_pred_1=test_the_finalmodel(final_model, X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared_finalmodel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        R-squared value.\n",
    "    \"\"\"\n",
    "    r_squared = None\n",
    "    # Code starts here\n",
    "    \n",
    "    # Code ends here \n",
    "    return r_squared\n",
    "#r-squared\n",
    "r_squared_final_model = calculate_r_squared_finalmodel(y_test, y_pred_1)\n",
    "print(r_squared_final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_finalmodel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        RMSE value.\n",
    "    \"\"\"\n",
    "    rmse = None\n",
    "    # Code starts here\n",
    "    \n",
    "\n",
    "    # Code ends here \n",
    "    \n",
    "    return rmse\n",
    "#rmse\n",
    "rmse_final_model = calculate_rmse_finalmodel(y_test, y_pred_1)\n",
    "print(rmse_final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Summarize the findings of the analysis and draw conclusions with PPT / PDF. (weightage - 5 marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- Can perform well with a small number of observations\n",
    "- Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------- **ASSESSMENT ENDS HERE** ---------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
